# -*- coding: utf-8 -*-
"""Semi_working_model_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AIV8njm1Vx2kCNhz-mO88hzEpdRx4anN

### **Install required libraries**
"""

!pip install pymupdf pytesseract opencv-python-headless pandas openpyxl torch torchvision pdf2image
!pip install git+https://github.com/facebookresearch/detectron2.git
!sudo apt install tesseract-ocr
!sudo apt-get install poppler-utils

"""### **Import necessary libraries**"""

import fitz  # PyMuPDF
import pytesseract
import cv2
import pandas as pd
import re
import torch
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2 import model_zoo
from detectron2.data.datasets import register_coco_instances
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog
import openpyxl
import pdf2image
import numpy as np
import os
import matplotlib.pyplot as plt

"""### **Setting Tesseract Executable Path**"""

pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'

!curl -L "https://app.roboflow.com/ds/IMUD6zdS5U?key=tKirO1edlW" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip

"""### **Register the dataset with Detectron2**"""

!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html
from detectron2.data.datasets import register_coco_instances

register_coco_instances("column_dataset_train", {}, "/content/train/_annotations.coco.json", "/content/train")
register_coco_instances("column_dataset_val", {}, "/content/valid/_annotations.coco.json", "/content/valid")

"""## **Verify the dataset registration**"""

# Import necessary libraries
from detectron2.data import DatasetCatalog, MetadataCatalog

# Clear the existing registrations (if any) before registering again
DatasetCatalog.clear()
MetadataCatalog.clear()

register_coco_instances("column_dataset_train", {}, "/content/train/_annotations.coco.json", "/content/train")
register_coco_instances("column_dataset_val", {}, "/content/valid/_annotations.coco.json", "/content/valid")

dataset_dicts = DatasetCatalog.get("column_dataset_train")
column_metadata = MetadataCatalog.get("column_dataset_train")

"""## **Visualize a few samples**"""

import random
for d in random.sample(dataset_dicts, 3):
    img = cv2.imread(d["file_name"])
    visualizer = Visualizer(img[:, :, ::-1], metadata=column_metadata, scale=0.5)
    vis = visualizer.draw_dataset_dict(d)
    plt.imshow(vis.get_image()[:, :, ::-1])
    plt.show()

"""### **Loading the Detectron2 Model**"""

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding
!pip install 'git+https://github.com/facebookresearch/detectron2.git'

# Set up the Detectron2 configuration and training
!pip install 'git+https://github.com/facebookresearch/detectron2.git'
import detectron2
from detectron2.config import get_cfg
from detectron2.engine import DefaultTrainer
from detectron2.evaluation import COCOEvaluator, inference_on_dataset # Import the missing function
from detectron2.data import build_detection_test_loader
from detectron2 import model_zoo
from detectron2.utils.visualizer import Visualizer
import os

# Set up Detectron2 configuration
cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("column_dataset_train",)
cfg.DATASETS.TEST = ("column_dataset_val",)
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")  # Initialize from pre-trained model
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.00025  # Pick a good LearningRate
cfg.SOLVER.MAX_ITER = 500    # Number of iterations
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # Number of regions per image used to train ROIHeads
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4  # Only one class: column_icon
cfg.MODEL.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
predictor = DefaultPredictor(cfg)

"""### **Augmentation function**"""

def custom_mapper(dataset_dict):
    dataset_dict = utils.copy.deepcopy(dataset_dict)
    image = utils.read_image(dataset_dict["file_name"], format="BGR")
    transform_list = [T.Resize((800, 800)), T.RandomRotation(angle=[-30, 30])]
    image, transforms = T.apply_transform_gens(transform_list, image)
    dataset_dict["image"] = torch.as_tensor(image.transpose(2, 0, 1).astype("float32"))
    annos = [utils.transform_instance_annotations(obj, transforms, image.shape[:2]) for obj in dataset_dict.pop("annotations")]
    dataset_dict["instances"] = utils.annotations_to_instances(annos, image.shape[:2])
    return dataset_dict

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("column_dataset_train",)
cfg.DATASETS.TEST = ("column_dataset_val",)
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")  # Initialize from pre-trained model
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.00025  # Pick a good LearningRate
cfg.SOLVER.MAX_ITER = 500    # Number of iterations
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # Number of regions per image used to train ROIHeads
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4  # Number of classes (column_icon, column_number, column_length, column_width)

from detectron2.data import MetadataCatalog
MetadataCatalog.get("your_dataset_name").thing_classes = ["columns-lenght-width", "column_icon", "column_lenght", "column_number", "column_width"]

# Explicitly set the device to CPU if no GPU is available
cfg.MODEL.DEVICE = "cpu" if not torch.cuda.is_available() else "cuda"

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = DefaultTrainer(cfg)
trainer.resume_or_load(resume=False)
trainer.train()

"""## **Evaluate the model**"""

# Evaluate the model
evaluator = COCOEvaluator("column_dataset_val", cfg, False, output_dir="./output/")
val_loader = build_detection_test_loader(cfg, "column_dataset_val") # Now the function is available
print(inference_on_dataset(trainer.model, val_loader, evaluator))

"""## **Inference**"""

# Set up the inference configuration
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")  # Path to the trained model
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set the threshold for this model
predictor = DefaultPredictor(cfg)

"""### **Define helper functions for inference and processing**"""

def preprocess_image_for_ocr(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    blurred = cv2.GaussianBlur(gray, (5, 5), 0)
    preprocessed = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
    return preprocessed

# Define class_names here, replace with your actual class names
class_names = ['column_icon', 'column_number', 'column_length', 'column_width']

def predict_columns(image_path):
    img = cv2.imread(image_path)
    outputs = predictor(img)
    instances = outputs["instances"].to("cpu")
    boxes = instances.pred_boxes if instances.has("pred_boxes") else None
    scores = instances.scores if instances.has("scores") else None
    classes = instances.pred_classes if instances.has("pred_classes") else None

    detected_objects = []
    for box, score, cls in zip(boxes, scores, classes):
        if score > 0.5:
            x1, y1, x2, y2 = box.numpy()
            detected_objects.append({
                'label': 'column_icon',
                'bbox': [x1, y1, x2, y2],
                'score': score.item()
            })
    return detected_objects

def extract_text_from_bbox(image, bbox):
    x1, y1, x2, y2 = map(int, bbox)
    roi = image[y1:y2, x1:x2]
    preprocessed = preprocess_image_for_ocr(roi)
    # Using config options to improve OCR accuracy
    config = '--psm 6 -c tessedit_char_whitelist=0123456789'
    text = pytesseract.image_to_string(preprocessed, config=config)  # Page segmentation mode 6 assumes a single uniform block of text
    return text.strip()

def process_pdf(pdf_path):
    images = pdf2image.convert_from_path(pdf_path)
    all_detected_objects = []

    for i, image in enumerate(images):
        image_path = f'temp_image_{i}.png'
        image.save(image_path)
        detected_icons = predict_columns(image_path)
        img = cv2.imread(image_path)

        for icon in detected_icons:
            x1, y1, x2, y2 = map(int, icon['bbox'])
            # Define regions around the icon to extract text for length and width
            regions = {
                'column_number': (x1, y2 + 10, x2, y2 + 65),  # Adjust based on layout
                'column_length': (x1, y2 + 90, x2, y2 + 110),  # Adjust based on layout
                'column_width': (x1, y2 + 115, x2, y2 + 165)  # Adjust based on layout
            }

            for label, region in regions.items():
                text = extract_text_from_bbox(img, region)
                all_detected_objects.append({
                    'label': label,
                    'bbox': region,
                    'text': text
                })

        os.remove(image_path)  # Clean up the temporary image file

    return all_detected_objects

"""### **Function to save data to Excel with formulas**"""

def save_to_excel_with_formulas(detected_objects, output_excel_path, formulas):
    data = {
        'Column Number': [],
        'Column Length': [],
        'Column Width': []
    }

    for obj in detected_objects:
        if obj['label'] == 'column_number':
            data['Column Number'].append(obj['text'])
        elif obj['label'] == 'column_length':
            data['Column Length'].append(obj['text'])
        elif obj['label'] == 'column_width':
            data['Column Width'].append(obj['text'])

    max_length = max(len(data['Column Number']), len(data['Column Length']), len(data['Column Width']))

    for key in data:
        data[key] += [None] * (max_length - len(data[key]))

    df = pd.DataFrame(data)
    df.to_excel(output_excel_path, index=False)

    # Open the Excel file to add formulas
    wb = openpyxl.load_workbook(output_excel_path)
    ws = wb.active

    # Add formulas to the Excel sheet
    for formula in formulas:
        ws[formula['cell']] = formula['formula']

    wb.save(output_excel_path)

"""### **Functions to Detect and Extract Text**"""

!apt-get install tesseract-ocr
!pip install pytesseract
import pytesseract

# Upload the PDF file
uploaded = files.upload()
pdf_path = list(uploaded.keys())[0]

# Process the PDF and extract column information
detected_objects = process_pdf(pdf_path)

# Save the extracted information to Excel with formulas
output_excel_path = pdf_path.replace('.pdf', '_columns_info.xlsx')
formulas = [
    {'cell': 'E2', 'formula': '=C2*D2'},  # Example formula to multiply Length and Width
    {'cell': 'F2', 'formula': '=C2+D2'}   # Example formula to add Length and Width
]
save_to_excel_with_formulas(detected_objects, output_excel_path, formulas)

print(f"Columns information extracted and saved to {output_excel_path}")

# Print the detected objects to verify
for obj in detected_objects:
    print(obj)

"""## **Visualize detections**"""

images = pdf2image.convert_from_path(pdf_path)
for i, image in enumerate(images):
    image_path = f'temp_image_{i}.png'
    image.save(image_path)
    img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    detected = predict_columns(image_path)
    v = Visualizer(img[:, :, ::-1], scale=0.8)
    v = v.draw_instance_predictions(detected)
    plt.imshow(v.get_image()[:, :, ::-1])
    plt.show()
    os.remove(image_path)  # Clean up the temporary image file

!pip install detectron2

# Previous imports
import cv2
import numpy as np
import matplotlib.pyplot as plt
import pdf2image
import os
# Add this import
from detectron2.utils.visualizer import Visualizer
from detectron2.structures import Instances, Boxes

# ... rest of the code

images = pdf2image.convert_from_path(pdf_path)
for i, image in enumerate(images):
    image_path = f'temp_image_{i}.png'
    image.save(image_path)
    img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    detected = predict_columns(image_path)

    # Check if 'detected' is a list and convert if necessary
    if isinstance(detected, list):
        # Assuming 'detected' is a list of dictionaries with prediction results
        # You might need to adjust this conversion based on the actual output of 'predict_columns'

        # Create an Instances object (replace 800, 1333 with image dimensions)
        instances = Instances((800, 1333))

        # Populate the Instances object with the prediction results
        # Replace the placeholders with actual values from 'detected'
        # Example: Assuming 'detected' is a list of dictionaries like [{'boxes': ..., 'scores': ..., 'classes': ...}, ...]
        pred_boxes = Boxes(torch.tensor([d['boxes'] for d in detected])) # Adjust based on the format of 'boxes'
        scores = torch.tensor([d['scores'] for d in detected]) # Adjust based on the format of 'scores'
        pred_classes = torch.tensor([d['classes'] for d in detected]) # Adjust based on the format of 'classes'

        instances.set('pred_boxes', pred_boxes)
        instances.set('scores', scores)
        instances.set('pred_classes', pred_classes)
        detected = instances  # Replace the list with the Instances object

    v = Visualizer(img[:, :, ::-1], scale=0.8)
    v = v.draw_instance_predictions(detected)  # Now 'detected' should be an Instances object
    plt.imshow(v.get_image()[:, :, ::-1])
    plt.show()
    os.remove(image_path)  # Clean up the temporary image file

import json

# Path to your JSON annotation file
json_file_path = "/content/test/_annotations.coco.json"

# Load the JSON file
with open(json_file_path) as f:
    data = json.load(f)

# Extract the categories
categories = data['categories']

# Count the number of classes
num_classes = len(categories)
print(f"Number of classes: {num_classes}")

# Print category names and IDs
for category in categories:
    print(f"ID: {category['id']}, Name: {category['name']}")